{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "caf93673",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sheha\\anaconda3\\envs\\dojo-env\\lib\\site-packages\\pandas\\core\\arrays\\masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a24ab43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#citation for the dataset\n",
    "# 1. Misra, Rishabh and Prahal Arora. \"Sarcasm Detection using News Headlines Dataset.\" AI Open (2023).\n",
    "# 2. Misra, Rishabh and Jigyasa Grover. \"Sculpting Data for ML: The first act of Machine Learning.\" ISBN 9798585463570 (2021).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c7ea004",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "with open(\"Sarcasm_Headlines_Dataset.json\", 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    for i in lines:\n",
    "        print(i)                          # optional - just print it out to explain the next 2 lines\n",
    "        i.replace(\"'\",\"\")              # this is to replace \" ' \" with \"\"\n",
    "        i.replace(\"/n\",\"\")          # this is to replace \"/n\" with \"\"\n",
    "        obj = json.loads(i)\n",
    "        data.append(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2771c3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences =[]\n",
    "labels =[]\n",
    "urls =[]\n",
    "\n",
    "for item in data:\n",
    "    sentences.append(item['headline'])\n",
    "    labels.append(item['is_sarcastic'])\n",
    "    urls.append(item['article_link'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5df03c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split to train and test data\n",
    "train_size = 20000\n",
    "\n",
    "train_sentences = sentences[0:train_size]\n",
    "test_sentences = sentences[train_size:]\n",
    "\n",
    "train_labels = labels[0:train_size]\n",
    "test_labels = labels[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed767c02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 328    1  799 3405 2404   47  389 2214    1    6 2614 8863    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0]\n",
      "(20000, 100)\n"
     ]
    }
   ],
   "source": [
    "#instantiate tokenizer\n",
    "vocab_size =10000\n",
    "tokenizer = Tokenizer(num_words = vocab_size, oov_token = \"<OOV>\")\n",
    "tokenizer.fit_on_texts(train_sentences)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "#create sequences for train data\n",
    "max_length = 100\n",
    "trunc_type='post'\n",
    "padding_type='post'\n",
    "\n",
    "train_sequences = tokenizer.texts_to_sequences(train_sentences)\n",
    "#padding for train data\n",
    "train_padded = pad_sequences(train_sequences, padding = padding_type,\n",
    "                             maxlen= max_length, truncating = trunc_type)\n",
    "\n",
    "#create sequences for test data\n",
    "test_sequences = tokenizer.texts_to_sequences(test_sentences)\n",
    "#padding for test data\n",
    "test_padded = pad_sequences(test_sequences, padding = padding_type,\n",
    "                             maxlen= max_length, truncating = trunc_type)\n",
    "\n",
    "print(train_padded[0])\n",
    "print(train_padded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18c7fc1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need this block to get it to work with TensorFlow 2.x\n",
    "import numpy as np\n",
    "train_padded = np.array(train_padded)\n",
    "train_labels = np.array(train_labels)\n",
    "test_padded = np.array(test_padded)\n",
    "test_labels = np.array(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e6bcecd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 100, 16)           160000    \n",
      "                                                                 \n",
      " global_average_pooling1d (G  (None, 16)               0         \n",
      " lobalAveragePooling1D)                                          \n",
      "                                                                 \n",
      " dense (Dense)               (None, 24)                408       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 25        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 160,433\n",
      "Trainable params: 160,433\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#instantiate the model\n",
    "embedding_dim = 16\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length = max_length),\n",
    "    tf.keras.layers.GlobalAveragePooling1D(),\n",
    "    tf.keras.layers.Dense(24, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid') \n",
    "])\n",
    "\n",
    "\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "59b417bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "625/625 - 3s - loss: 0.6745 - accuracy: 0.5694 - val_loss: 0.6217 - val_accuracy: 0.6105 - 3s/epoch - 4ms/step\n",
      "Epoch 2/30\n",
      "625/625 - 2s - loss: 0.4591 - accuracy: 0.8152 - val_loss: 0.3979 - val_accuracy: 0.8302 - 2s/epoch - 3ms/step\n",
      "Epoch 3/30\n",
      "625/625 - 2s - loss: 0.3206 - accuracy: 0.8722 - val_loss: 0.3595 - val_accuracy: 0.8468 - 2s/epoch - 3ms/step\n",
      "Epoch 4/30\n",
      "625/625 - 2s - loss: 0.2680 - accuracy: 0.8929 - val_loss: 0.3496 - val_accuracy: 0.8523 - 2s/epoch - 3ms/step\n",
      "Epoch 5/30\n",
      "625/625 - 2s - loss: 0.2315 - accuracy: 0.9115 - val_loss: 0.3501 - val_accuracy: 0.8544 - 2s/epoch - 3ms/step\n",
      "Epoch 6/30\n",
      "625/625 - 2s - loss: 0.2023 - accuracy: 0.9224 - val_loss: 0.3478 - val_accuracy: 0.8544 - 2s/epoch - 3ms/step\n",
      "Epoch 7/30\n",
      "625/625 - 2s - loss: 0.1808 - accuracy: 0.9315 - val_loss: 0.3607 - val_accuracy: 0.8526 - 2s/epoch - 3ms/step\n",
      "Epoch 8/30\n",
      "625/625 - 2s - loss: 0.1620 - accuracy: 0.9389 - val_loss: 0.3710 - val_accuracy: 0.8544 - 2s/epoch - 3ms/step\n",
      "Epoch 9/30\n",
      "625/625 - 2s - loss: 0.1444 - accuracy: 0.9480 - val_loss: 0.3872 - val_accuracy: 0.8533 - 2s/epoch - 3ms/step\n",
      "Epoch 10/30\n",
      "625/625 - 2s - loss: 0.1336 - accuracy: 0.9521 - val_loss: 0.4489 - val_accuracy: 0.8326 - 2s/epoch - 3ms/step\n",
      "Epoch 11/30\n",
      "625/625 - 2s - loss: 0.1201 - accuracy: 0.9591 - val_loss: 0.4395 - val_accuracy: 0.8433 - 2s/epoch - 3ms/step\n",
      "Epoch 12/30\n",
      "625/625 - 2s - loss: 0.1104 - accuracy: 0.9618 - val_loss: 0.4616 - val_accuracy: 0.8404 - 2s/epoch - 3ms/step\n",
      "Epoch 13/30\n",
      "625/625 - 2s - loss: 0.1006 - accuracy: 0.9657 - val_loss: 0.4723 - val_accuracy: 0.8447 - 2s/epoch - 3ms/step\n",
      "Epoch 14/30\n",
      "625/625 - 2s - loss: 0.0921 - accuracy: 0.9686 - val_loss: 0.5154 - val_accuracy: 0.8366 - 2s/epoch - 3ms/step\n",
      "Epoch 15/30\n",
      "625/625 - 2s - loss: 0.0847 - accuracy: 0.9722 - val_loss: 0.5260 - val_accuracy: 0.8398 - 2s/epoch - 3ms/step\n",
      "Epoch 16/30\n",
      "625/625 - 2s - loss: 0.0778 - accuracy: 0.9751 - val_loss: 0.5561 - val_accuracy: 0.8374 - 2s/epoch - 3ms/step\n",
      "Epoch 17/30\n",
      "625/625 - 2s - loss: 0.0727 - accuracy: 0.9762 - val_loss: 0.5936 - val_accuracy: 0.8348 - 2s/epoch - 3ms/step\n",
      "Epoch 18/30\n",
      "625/625 - 2s - loss: 0.0659 - accuracy: 0.9787 - val_loss: 0.6156 - val_accuracy: 0.8319 - 2s/epoch - 3ms/step\n",
      "Epoch 19/30\n",
      "625/625 - 2s - loss: 0.0603 - accuracy: 0.9808 - val_loss: 0.6472 - val_accuracy: 0.8313 - 2s/epoch - 4ms/step\n",
      "Epoch 20/30\n",
      "625/625 - 2s - loss: 0.0561 - accuracy: 0.9828 - val_loss: 0.7113 - val_accuracy: 0.8267 - 2s/epoch - 3ms/step\n",
      "Epoch 21/30\n",
      "625/625 - 2s - loss: 0.0513 - accuracy: 0.9844 - val_loss: 0.7158 - val_accuracy: 0.8292 - 2s/epoch - 3ms/step\n",
      "Epoch 22/30\n",
      "625/625 - 2s - loss: 0.0485 - accuracy: 0.9853 - val_loss: 0.7506 - val_accuracy: 0.8249 - 2s/epoch - 3ms/step\n",
      "Epoch 23/30\n",
      "625/625 - 2s - loss: 0.0443 - accuracy: 0.9865 - val_loss: 0.7826 - val_accuracy: 0.8247 - 2s/epoch - 3ms/step\n",
      "Epoch 24/30\n",
      "625/625 - 2s - loss: 0.0402 - accuracy: 0.9883 - val_loss: 0.8173 - val_accuracy: 0.8216 - 2s/epoch - 3ms/step\n",
      "Epoch 25/30\n",
      "625/625 - 2s - loss: 0.0369 - accuracy: 0.9890 - val_loss: 0.9195 - val_accuracy: 0.8170 - 2s/epoch - 3ms/step\n",
      "Epoch 26/30\n",
      "625/625 - 2s - loss: 0.0336 - accuracy: 0.9905 - val_loss: 0.8846 - val_accuracy: 0.8211 - 2s/epoch - 3ms/step\n",
      "Epoch 27/30\n",
      "625/625 - 2s - loss: 0.0328 - accuracy: 0.9903 - val_loss: 0.9503 - val_accuracy: 0.8195 - 2s/epoch - 3ms/step\n",
      "Epoch 28/30\n",
      "625/625 - 2s - loss: 0.0298 - accuracy: 0.9914 - val_loss: 0.9601 - val_accuracy: 0.8183 - 2s/epoch - 4ms/step\n",
      "Epoch 29/30\n",
      "625/625 - 2s - loss: 0.0273 - accuracy: 0.9920 - val_loss: 1.0648 - val_accuracy: 0.8106 - 2s/epoch - 3ms/step\n",
      "Epoch 30/30\n",
      "625/625 - 2s - loss: 0.0250 - accuracy: 0.9923 - val_loss: 1.0475 - val_accuracy: 0.8144 - 2s/epoch - 3ms/step\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 30\n",
    "history = model.fit(train_padded, train_labels, epochs=num_epochs, validation_data=(test_padded, test_labels), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "16d0e22d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 85ms/step\n",
      "[[9.0806085e-01]\n",
      " [5.8077103e-06]]\n"
     ]
    }
   ],
   "source": [
    "#test for new sentences\n",
    "sentence = [\"granny starting to fear spiders in the garden might be real\", \n",
    "            \"game of thrones season finale showing this sunday night\"]\n",
    "sequences = tokenizer.texts_to_sequences(sentence)\n",
    "padded = pad_sequences(sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
    "print(model.predict(padded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73081faa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (dojo-env)",
   "language": "python",
   "name": "dojo-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
